{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, LSTM\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.regularizers import l2\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from IPython.display import SVG, Image\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "[Credit Card Fraud Detection - Kaggle](https://www.kaggle.com/dalpozz/creditcardfraud)에서 다운받을수 있습니다.\n",
    "\n",
    "데이터는 2013년 유럽 카드회사의 이틀동안 일어난 transactions에 관한 것이며, <br>\n",
    "492건의 frauds 가 284,807건의 transactions중에 일어 났습니다.\n",
    "\n",
    "Class에서 1은 fraud를 뜻하며, 0은 아닌것을 말합니다.\n",
    "\n",
    "Time데이터는 첫번재 Column으로부터 몇초 이후에 발생한 transaction이라는 뜻입니다. <br>\n",
    "나머지 데이터들은 PCA의 규제에 의해서 어떤 데이터인지 밝히지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/dataset/credit-card-fraud-detection/creditcard.csv')\n",
    "\n",
    "# Preprocessing Amount\n",
    "amt_scale = MinMaxScaler()\n",
    "data['NormAmount'] =  amt_scale.fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "\n",
    "# Split Train and Test Data\n",
    "X = data.drop(['Time', 'Amount', 'Class'], axis=1).as_matrix()\n",
    "Y = data['Class'].as_matrix()\n",
    "\n",
    "# Standardization\n",
    "scale_x = MinMaxScaler()\n",
    "X = scale_x.fit_transform(X)\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.25, random_state=1)\n",
    "\n",
    "fraud_test_y = test_y == 1\n",
    "fraud_test_x = test_x[fraud_test_y]\n",
    "fraud_test_y = test_y[fraud_test_y]\n",
    "\n",
    "train_category_y = np_utils.to_categorical(train_y)\n",
    "test_category_y = np_utils.to_categorical(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the number of fraud transactions in training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of Fraud transactions in Training Data: 381\n",
      "The number of Fraud transactions in Test Data: 111\n"
     ]
    }
   ],
   "source": [
    "print('The number of Fraud transactions in Training Data:', train_y[train_y == 1].shape[0])\n",
    "print('The number of Fraud transactions in Test Data:',  test_y[test_y == 1].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the target classes\n",
    "\n",
    "fraud transactions이 492개밖에 되지 않기 때문에, 일반적인 classification algorithm으로 돌리면 물론 정확도는 매우 높게 나오지만.. \n",
    "실상은 1에 해당하는 fraud transactions에서는 대부분 틀릴 가능성이 매우 높습니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(data['Class'], sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "\n",
    "resampling에는 여러가지 방법이 있습니다. \n",
    "\n",
    "1. Over Sampling: SMOTE (Synthetic Minority Over-Sampling Technique)\n",
    "2. Under Sampling\n",
    "\n",
    "아래의 resample function에서는 5:5의 비율로 under sampling을 해줍니다.<br>\n",
    "resample을 하면서 시간관계가 어차피 깨지기 때문에 (사실 각각의 transactions들 사이에 상관관계가 있는지도 모르겠음)<br>\n",
    "shuffle을 통해서 train되는 데이터를 augment해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resampled_train_x: (762, 29)\n",
      "resampled_train_y: (762,)\n"
     ]
    }
   ],
   "source": [
    "def resample(X, Y, ratio=1.):\n",
    "    index = np.arange(Y.shape[0])\n",
    "    fraud_indices = index[Y == 1]\n",
    "    normal_indices = index[Y == 0]\n",
    "    normal_n = int(len(fraud_indices) * ratio)\n",
    "    \n",
    "    random_normal_indices = np.random.permutation(normal_indices)[:normal_n]\n",
    "    \n",
    "    sample_indices = np.concatenate([fraud_indices, random_normal_indices])\n",
    "    np.random.shuffle(sample_indices)\n",
    "    sample_indices = np.array(sample_indices)\n",
    "    \n",
    "    sample_x = X[sample_indices]\n",
    "    sample_y = Y[sample_indices]\n",
    "    return sample_x, sample_y\n",
    "\n",
    "resampled_train_x, resampled_train_y = resample(train_x, train_y)\n",
    "\n",
    "print('resampled_train_x:', resampled_train_x.shape)\n",
    "print('resampled_train_y:', resampled_train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "전체적으로 0.99% accuracy를 보이지만, 실제 fraud data만 test를 했을때는 0.57%로.. 실질적으로 못맞추는 수준입니다.<br>\n",
    "사실 일반적인 알고리즘으로 학습시키기 위해서는 over sampling (SMOTE 같은) 또는 under sampling이 필요합니다.<br>\n",
    "sampling을 통해서 skewed data를 보정하는 것입니다.\n",
    "\n",
    "#### resample 없이 데이터 학습뒤 예측하면.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99908710429482317"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg = LogisticRegression()\n",
    "lg.fit(train_x, train_y)\n",
    "predicted_y = lg.predict(test_x)\n",
    "accuracy_score(test_y, predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51351351351351349"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_y = lg.predict(fraud_test_x)\n",
    "accuracy_score(fraud_test_y, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### resampled data로 학습뒤 예측하면..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9970506446448133"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg = LogisticRegression()\n",
    "lg.fit(*resample(train_x, train_y))\n",
    "\n",
    "predicted_y = lg.predict(test_x)\n",
    "accuracy_score(test_y, predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77477477477477474"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_y = lg.predict(fraud_test_x)\n",
    "accuracy_score(fraud_test_y, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "#### resample 없이 데이터 학습뒤 예측하면.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99925563888654811"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier(max_depth=10, criterion='entropy')\n",
    "dtc.fit(train_x, train_y)\n",
    "predicted_y = dtc.predict(test_x)\n",
    "accuracy_score(test_y, predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72972972972972971"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_y = dtc.predict(fraud_test_x)\n",
    "accuracy_score(fraud_test_y, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### resampled data로 학습뒤 예측하면..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.899974719811\n"
     ]
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier(max_depth=10, criterion='entropy')\n",
    "dtc.fit(*resample(train_x, train_y))\n",
    "predicted_y = dtc.predict(test_x)\n",
    "print(accuracy_score(test_y, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95495495495495497"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_y = dtc.predict(fraud_test_x)\n",
    "accuracy_score(fraud_test_y, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Keras\n",
    "\n",
    "하.. 드디어 딥러닝으로.. 해보면 어떤 결과가 나올 것인가.. Sampling VS UnSampling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.1\n",
    "set_session(tf.InteractiveSession(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               7680      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 160)               41120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               20608     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 96)                12384     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 96)                384       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 97        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 84,449\n",
      "Trainable params: 83,169\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def generate_model():\n",
    "    np.random.seed(0)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=29))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(160))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Dense(128))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(96))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# # Visualization\n",
    "model = generate_model()\n",
    "model.summary()\n",
    "# SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### resample 없이 데이터 학습뒤 예측하면.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "27s - loss: 0.0100 - acc: 0.9980\n",
      "Epoch 2/10\n",
      "27s - loss: 0.0041 - acc: 0.9993\n",
      "Epoch 3/10\n",
      "27s - loss: 0.0038 - acc: 0.9993\n",
      "Epoch 4/10\n",
      "27s - loss: 0.0036 - acc: 0.9994\n",
      "Epoch 5/10\n",
      "27s - loss: 0.0034 - acc: 0.9993\n",
      "Epoch 6/10\n",
      "27s - loss: 0.0035 - acc: 0.9993\n",
      "Epoch 7/10\n",
      "27s - loss: 0.0034 - acc: 0.9994\n",
      "Epoch 8/10\n",
      "27s - loss: 0.0034 - acc: 0.9994\n",
      "Epoch 9/10\n",
      "27s - loss: 0.0034 - acc: 0.9994\n",
      "Epoch 10/10\n",
      "27s - loss: 0.0033 - acc: 0.9994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f01d2528898>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = generate_model()\n",
    "model.fit(train_x, train_y, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999311817084\n"
     ]
    }
   ],
   "source": [
    "predicted_y = model.predict(test_x)\n",
    "predicted_y = predicted_y.reshape(predicted_y.shape[0])\n",
    "predicted_y = np.where(predicted_y >= 0.5, 1, 0)\n",
    "print(accuracy_score(test_y, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74774774774774777"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_y = model.predict(fraud_test_x)\n",
    "predicted_y = predicted_y.reshape(predicted_y.shape[0])\n",
    "predicted_y = np.where(predicted_y >= 0.5, 1, 0)\n",
    "accuracy_score(fraud_test_y, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### resampled data로 학습뒤 예측하면..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1] epoch:50 loss:0.1747   acc:0.9351  \n",
      "[ 2] epoch:50 loss:0.1154   acc:0.957   \n",
      "[ 3] epoch:50 loss:0.09408  acc:0.9642  \n",
      "[ 4] epoch:50 loss:0.09055  acc:0.9646  \n",
      "[ 5] epoch:50 loss:0.06973  acc:0.9732  \n",
      "[ 6] epoch:50 loss:0.06866  acc:0.9734  \n",
      "[ 7] epoch:50 loss:0.05985  acc:0.9764  \n",
      "[ 8] epoch:50 loss:0.05556  acc:0.9785  \n",
      "[ 9] epoch:50 loss:0.05241  acc:0.9789  \n",
      "[10] epoch:50 loss:0.04598  acc:0.9827  \n",
      "[11] epoch:50 loss:0.04265  acc:0.9832  \n",
      "[12] epoch:50 loss:0.03687  acc:0.9853  \n",
      "[13] epoch:50 loss:0.04159  acc:0.9841  \n",
      "[14] epoch:50 loss:0.03852  acc:0.9851  \n",
      "[15] epoch:50 loss:0.03585  acc:0.9861  \n",
      "[16] epoch:50 loss:0.04974  acc:0.9817  \n",
      "[17] epoch:50 loss:0.03046  acc:0.9885  \n",
      "[18] epoch:50 loss:0.03561  acc:0.9875  \n",
      "[19] epoch:50 loss:0.03836  acc:0.986   \n",
      "[20] epoch:50 loss:0.02893  acc:0.9888  \n",
      "[21] epoch:50 loss:0.0328   acc:0.9882  \n",
      "[22] epoch:50 loss:0.02712  acc:0.9898  \n",
      "[23] epoch:50 loss:0.03774  acc:0.9865  \n",
      "[24] epoch:50 loss:0.02368  acc:0.991   \n",
      "[25] epoch:50 loss:0.02782  acc:0.9902  \n",
      "[26] epoch:50 loss:0.02484  acc:0.9912  \n",
      "[27] epoch:50 loss:0.02646  acc:0.9899  \n",
      "[28] epoch:50 loss:0.03993  acc:0.9902  \n",
      "[29] epoch:50 loss:0.03121  acc:0.9904  \n",
      "[30] epoch:50 loss:0.02874  acc:0.9895  \n",
      "[31] epoch:50 loss:0.02508  acc:0.9908  \n",
      "[32] epoch:50 loss:0.02123  acc:0.9922  \n",
      "[33] epoch:50 loss:0.02777  acc:0.9896  \n",
      "[34] epoch:50 loss:0.02843  acc:0.9895  \n",
      "[35] epoch:50 loss:0.0226   acc:0.9919  \n",
      "[36] epoch:50 loss:0.0198   acc:0.9927  \n",
      "[37] epoch:50 loss:0.01881  acc:0.9933  \n",
      "[38] epoch:50 loss:0.02115  acc:0.9926  \n",
      "[39] epoch:50 loss:0.02356  acc:0.9915  \n",
      "[40] epoch:50 loss:0.0213   acc:0.993   \n",
      "[41] epoch:50 loss:0.0213   acc:0.9927  \n",
      "[42] epoch:50 loss:0.01553  acc:0.9946  \n",
      "[43] epoch:50 loss:0.02371  acc:0.9913  \n",
      "[44] epoch:50 loss:0.01749  acc:0.9938  \n",
      "[45] epoch:50 loss:0.02064  acc:0.9931  \n",
      "[46] epoch:50 loss:0.0221   acc:0.9923  \n",
      "[47] epoch:50 loss:0.02131  acc:0.9923  \n",
      "[48] epoch:50 loss:0.02364  acc:0.9917  \n",
      "[49] epoch:50 loss:0.0204   acc:0.9928  \n",
      "[50] epoch:50 loss:0.02468  acc:0.9928  \n",
      "[51] epoch:50 loss:0.02787  acc:0.9907  \n",
      "[52] epoch:50 loss:0.02036  acc:0.9931  \n",
      "[53] epoch:50 loss:0.015    acc:0.9948  \n",
      "[54] epoch:50 loss:0.01952  acc:0.993   \n",
      "[55] epoch:50 loss:0.02051  acc:0.9928  \n",
      "[56] epoch:50 loss:0.01304  acc:0.9953  \n",
      "[57] epoch:50 loss:0.02302  acc:0.9912  \n",
      "[58] epoch:50 loss:0.02222  acc:0.9923  \n",
      "[59] epoch:50 loss:0.02169  acc:0.9924  \n",
      "[60] epoch:50 loss:0.01611  acc:0.9939  \n",
      "[61] epoch:50 loss:0.01526  acc:0.9949  \n",
      "[62] epoch:50 loss:0.02123  acc:0.9926  \n",
      "[63] epoch:50 loss:0.02727  acc:0.9909  \n",
      "[64] epoch:50 loss:0.01674  acc:0.9944  \n",
      "[65] epoch:50 loss:0.02663  acc:0.9933  \n",
      "[66] epoch:50 loss:0.01323  acc:0.9957  \n",
      "[67] epoch:50 loss:0.01539  acc:0.9948  \n",
      "[68] epoch:50 loss:0.01403  acc:0.9949  \n",
      "[69] epoch:50 loss:0.01818  acc:0.994   \n",
      "[70] epoch:50 loss:0.02583  acc:0.992   \n",
      "[71] epoch:50 loss:0.0177   acc:0.9939  \n",
      "[72] epoch:50 loss:0.02266  acc:0.9922  \n",
      "[73] epoch:50 loss:0.01834  acc:0.9937  \n",
      "[74] epoch:50 loss:0.02084  acc:0.9924  \n",
      "[75] epoch:50 loss:0.0177   acc:0.9936  \n",
      "[76] epoch:50 loss:0.01589  acc:0.9946  \n",
      "[77] epoch:50 loss:0.0155   acc:0.9946  \n",
      "[78] epoch:50 loss:0.01654  acc:0.9937  \n",
      "[79] epoch:50 loss:0.01753  acc:0.9938  \n",
      "[80] epoch:50 loss:0.02277  acc:0.9924  \n",
      "[81] epoch:50 loss:0.0181   acc:0.9938  \n",
      "[82] epoch:50 loss:0.01394  acc:0.9949  \n",
      "[83] epoch:50 loss:0.01233  acc:0.9963  \n",
      "[84] epoch:50 loss:0.02108  acc:0.9949  \n",
      "[85] epoch:50 loss:0.01739  acc:0.9942  \n",
      "[86] epoch:50 loss:0.01714  acc:0.9941  \n",
      "[87] epoch:50 loss:0.01889  acc:0.994   \n",
      "[88] epoch:50 loss:0.01492  acc:0.9947  \n",
      "[89] epoch:50 loss:0.01542  acc:0.9948  \n",
      "[90] epoch:50 loss:0.01788  acc:0.994   \n",
      "[91] epoch:50 loss:0.02462  acc:0.9915  \n",
      "[92] epoch:50 loss:0.01342  acc:0.9953  \n",
      "[93] epoch:50 loss:0.01917  acc:0.9935  \n",
      "[94] epoch:50 loss:0.01176  acc:0.9957  \n",
      "[95] epoch:50 loss:0.01745  acc:0.9946  \n",
      "[96] epoch:50 loss:0.01573  acc:0.9945  \n",
      "[97] epoch:50 loss:0.0196   acc:0.9933  \n",
      "[98] epoch:50 loss:0.01235  acc:0.9955  \n",
      "[99] epoch:50 loss:0.01158  acc:0.9957  \n",
      "[100] epoch:50 loss:0.01595  acc:0.9941  \n"
     ]
    }
   ],
   "source": [
    "# # Visualization\n",
    "model = generate_model()\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=10)\n",
    "\n",
    "for i in range(100):\n",
    "    _train_data = resample(train_x, train_y, ratio=1)\n",
    "        \n",
    "    history = model.fit(*_train_data,\n",
    "                        verbose=0, \n",
    "                        epochs=50,)\n",
    "#                         callbacks=[early_stopping])\n",
    "    loss = np.mean(history.history.get('loss'))\n",
    "    acc = np.mean(history.history.get('acc'))\n",
    "    epoch = len(history.epoch)\n",
    "    print(f'[{i+1:2}] epoch:{epoch:<2} loss:{loss:<8.4} acc:{acc:<8.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.997457936575\n"
     ]
    }
   ],
   "source": [
    "predicted_y = model.predict(test_x)\n",
    "predicted_y = predicted_y.reshape(predicted_y.shape[0])\n",
    "predicted_y = np.where(predicted_y >= 0.5, 1, 0)\n",
    "print(accuracy_score(test_y, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8288288288288288"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_y = model.predict(fraud_test_x)\n",
    "predicted_y = predicted_y.reshape(predicted_y.shape[0])\n",
    "predicted_y = np.where(predicted_y >= 0.5, 1, 0)\n",
    "accuracy_score(fraud_test_y, predicted_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
